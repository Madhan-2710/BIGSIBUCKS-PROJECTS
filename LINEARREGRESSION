import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.datasets import fetch_openml

# Load the Boston Housing dataset
boston = fetch_openml(name='Boston', version=1, as_frame=True)
df = boston.frame

# Display the first few rows to verify the data
print(df.head())

# Inspect column names
print("Column names:", df.columns)

# Check for alternative names for the target variable
potential_target_names = ['MEDV', 'target']

# Identify the correct target variable name
target_column = None
for name in potential_target_names:
    if name in df.columns:
        target_column = name
        break

if target_column is None:
    raise ValueError("Target variable not found in the dataset. Check column names.")

# Create 5 new random features
np.random.seed(42)
for i in range(1, 6):
    df[f'NEW_FEATURE_{i}'] = np.random.rand(len(df))

# Select features and target
X = df.drop(target_column, axis=1)  # Drop the target variable
y = df[target_column]  # Target variable

# Handle categorical columns appropriately
categorical_columns = X.select_dtypes(include=['category']).columns
if not categorical_columns.empty:
    print(f"Converting categorical columns {categorical_columns} to numeric...")
    X[categorical_columns] = X[categorical_columns].astype('category').apply(lambda x: x.cat.codes)

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the linear regression model
model = LinearRegression()
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate the metrics
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(f'Mean Squared Error: {mse}')
print(f'RÂ² Score: {r2}')

# Plot Actual vs Predicted Prices and Residuals together
plt.figure(figsize=(10, 6))

# Plot Actual vs Predicted Prices
plt.scatter(y_test, y_pred, color='blue', label='Actual vs Predicted')

# Plot Residuals
residuals = y_test - y_pred
plt.scatter(y_pred, residuals, color='red', label='Residuals')

# Add a diagonal line for reference in Actual vs Predicted Prices
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], color='black', linestyle='-', linewidth=2)

# Add labels and title
plt.xlabel('Actual Prices / Predicted Prices')
plt.ylabel('Residuals')
plt.title('Actual vs Predicted Prices and Residuals')
plt.legend()

# Show plot
plt.grid(True)
plt.tight_layout()
plt.show()

# Additional plots

# Distribution of the Target Variable
plt.figure(figsize=(10, 6))
plt.hist(y, bins=30, color='skyblue', edgecolor='black')
plt.xlabel('House Prices')
plt.ylabel('Frequency')
plt.title('Distribution of House Prices')
plt.grid(True)
plt.show()

# Correlation Matrix
correlation_matrix = df.corr()
plt.figure(figsize=(12, 10))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', linewidths=0.5)
plt.title('Correlation Matrix')
plt.show()

# Feature Importance
coefficients = pd.Series(model.coef_, index=X.columns)
coefficients = coefficients.sort_values()
plt.figure(figsize=(10, 6))
coefficients.plot(kind='barh')
plt.title('Feature Importance')
plt.xlabel('Coefficient Value')
plt.ylabel('Feature')
plt.grid(True)
plt.show()

# Pair Plot
# Selecting a subset of features for the pair plot to avoid overcrowding
subset_features = df.columns[:5].tolist() + [target_column]  # Select the first 5 features plus the target
sns.pairplot(df[subset_features])
plt.suptitle('Pair Plot of Selected Features and House Prices', y=1.02)
plt.show()
